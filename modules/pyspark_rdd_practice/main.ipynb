{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc23608",
   "metadata": {},
   "source": [
    "# RDD Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d81bdfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09c2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run locally\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0167c61",
   "metadata": {},
   "source": [
    "Simple `map()` example with lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41b961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.8, 17.4, 16.2, 16.1, 14.8, 15.1, 15.0]\n"
     ]
    }
   ],
   "source": [
    "temps_f = [64.0, 63.3, 61.2, 61.0, 58.7, 59.2, 59.0]\n",
    "\n",
    "temps_f_rdd = spark.sparkContext.parallelize(temps_f)\n",
    "temps_c_rdd = temps_f_rdd.map(lambda t: round((t - 32) * 5 / 9, 1))\n",
    "\n",
    "temps_c = temps_c_rdd.collect()\n",
    "print(temps_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d9af2",
   "metadata": {},
   "source": [
    "You don't need a lambda, normal functions work as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e62e54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.8, 17.4, 16.2, 16.1, 14.8, 15.1, 15.0]\n"
     ]
    }
   ],
   "source": [
    "# works with normal (not lambda) funcs as well:\n",
    "def cels_to_fahr(cels):\n",
    "    fahr = (cels - 32) * 5 / 9\n",
    "    return round(fahr, 1)\n",
    "\n",
    "\n",
    "temps_f = spark.sparkContext.parallelize([64.0, 63.3, 61.2, 61.0, 58.7, 59.2, 59.0])\n",
    "print(temps_f.map(cels_to_fahr).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b87f1",
   "metadata": {},
   "source": [
    "PySpark is lazily evaluated, so only certain functions will actually trigger a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79bfe66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined illegal transformation (fine)\n",
      "Calling collect() on the result (errors)\n",
      "Error:\n",
      "  Py4JJavaError('An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n', JavaObject id=o159)\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3])\n",
    "\n",
    "# this transformation is clearly illegal\n",
    "rdd_messed_up = rdd.map(lambda x: x / 0 + \"bad\")\n",
    "print(\"Defined illegal transformation (fine)\")\n",
    "\n",
    "# ...but it won't error until we actually collect() the result:\n",
    "try:\n",
    "    print(\"Calling collect() on the result (errors)\")\n",
    "    rdd_messed_up.collect()\n",
    "except Py4JJavaError as e:\n",
    "    print(f\"Error:\\n  {repr(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5bea3",
   "metadata": {},
   "source": [
    "Another example of lazy evaluation. Here the \"expensive\" computation never actually happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "939001bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This RDD exists despite never being evaluated:\n",
      "  PythonRDD[18] at RDD at PythonRDD.scala:56\n"
     ]
    }
   ],
   "source": [
    "def expensive_operation(input):\n",
    "    time.sleep(5000000)  # sleep for many many seconds\n",
    "    return input + 1\n",
    "\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3]).map(expensive_operation)\n",
    "print(\"This RDD exists despite never being evaluated:\")\n",
    "print(\"  \" + repr(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654c826",
   "metadata": {},
   "source": [
    "Calculating averages with `reduce()` and `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a065d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score is 83.6\n"
     ]
    }
   ],
   "source": [
    "scores = [72, 87, 92, 76, 83, 91, 63, 88, 99, 85]\n",
    "\n",
    "scores_rdd = spark.sparkContext.parallelize(scores)\n",
    "\n",
    "total = scores_rdd.reduce(lambda x, y: x + y)\n",
    "count = scores_rdd.count()\n",
    "print(f\"Average score is {total / count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d77a1",
   "metadata": {},
   "source": [
    "TODO: `broadcast()` and `accumulator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d383e",
   "metadata": {},
   "source": [
    "More examples (none of these are important imo):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b344d",
   "metadata": {},
   "source": [
    "`countByKey()` and `countByValue()` measure the frequencies of things and return a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80b8373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count by value:  defaultdict(<class 'int'>, {'Alice': 1, 'Bob': 3, 'Charlie': 1, 'Eve': 2, 'Eggplant': 1})\n",
      "count by key:  defaultdict(<class 'int'>, {'A': 1, 'B': 3, 'C': 1, 'E': 3})\n"
     ]
    }
   ],
   "source": [
    "names = [\"Alice\", \"Bob\", \"Bob\", \"Bob\", \"Charlie\", \"Eve\", \"Eve\", \"Eggplant\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(names)\n",
    "# basic counter\n",
    "print(\"count by value: \", rdd.countByValue())\n",
    "# first char in each string is the 'key'\n",
    "# usually this would be used with tuples\n",
    "print(\"count by key: \", rdd.countByKey())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b49f5f",
   "metadata": {},
   "source": [
    "Perform aggregates on paired data with `reduceByKey()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1282e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jerry', 83.5), ('Jimbob', 76.25)]\n"
     ]
    }
   ],
   "source": [
    "student_scores = [\n",
    "    (\"Jerry\", 72),\n",
    "    (\"Jerry\", 87),\n",
    "    (\"Jerry\", 92),\n",
    "    (\"Jerry\", 76),\n",
    "    (\"Jerry\", 83),\n",
    "    (\"Jerry\", 91),\n",
    "    (\"Jimbob\", 63),\n",
    "    (\"Jimbob\", 88),\n",
    "    (\"Jimbob\", 69),\n",
    "    (\"Jimbob\", 85),\n",
    "]\n",
    "\n",
    "score_rdd = spark.sparkContext.parallelize(student_scores)\n",
    "total_rdd = score_rdd.reduceByKey(lambda x, y: x + y)\n",
    "count_rdd = score_rdd.countByKey()\n",
    "scores_rdd = total_rdd.map(lambda pair: (pair[0], pair[1] / count_rdd[pair[0]]))\n",
    "print(scores_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08205d47",
   "metadata": {},
   "source": [
    "`flatMap()` sounds simple but it's kinda weird imo. You can think of it as turning every row into multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1975c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array flattened: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Phrases flattened: ['the', 'quick', 'brown', 'fox', 'the', 'lazy', 'brown', 'dog']\n"
     ]
    }
   ],
   "source": [
    "data_2d = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "data_phrases = [[\"the quick brown fox\"], [\"the lazy brown dog\"]]\n",
    "\n",
    "rdd_2d = spark.sparkContext.parallelize(data_2d)\n",
    "rdd_phrases = spark.sparkContext.parallelize(data_phrases)\n",
    "\n",
    "flat_2d = rdd_2d.flatMap(lambda row: row)\n",
    "flat_phrases = rdd_phrases.flatMap(lambda row: row[0].split())\n",
    "\n",
    "print(\"2D array flattened:\", flat_2d.collect())\n",
    "print(\"Phrases flattened:\", flat_phrases.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
